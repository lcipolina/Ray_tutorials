{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi4V56Gn7kbn0l52kCpvD+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcipolina/Ray_tutorials/blob/main/TorchMultiCategorical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of issues in the TorchMuliCategoricalDistribution"
      ],
      "metadata": {
        "id": "sJdZzewyipxr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S43F79fTf_z_",
        "outputId": "35d1a476-0f98-40fa-d177-0e36b6680675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m836.9/836.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install ray[rllib] --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "6dtOt5bXjcOO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an environment with action space MultiDiscrete"
      ],
      "metadata": {
        "id": "KcQ5EGIojoDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MyCustomEnv_gymnasium(gym.Env):\n",
        "\n",
        "\n",
        "    def __init__(self, config_dict):\n",
        "        super(MyCustomEnv_gymnasium, self).__init__()\n",
        "\n",
        "        self.num_agents = config_dict.get('num_agents',2)\n",
        "        self.n          = config_dict.get('num_samples', 100)\n",
        "        self.priors     = config_dict.get('priors',[0.5, 0.5])\n",
        "        self.reservation_prices = config_dict.get('reservation_prices', [50]* self.num_agents)\n",
        "\n",
        "        # Define action and observation space\n",
        "        self.action_space      = gym.spaces.MultiDiscrete([self.n + 1]*self.num_agents)           # MultiDiscrete is mapped to Categorical distribution.\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=self.n, shape=(2 * self.num_agents,))\n",
        "\n",
        "        self.state = None\n",
        "        self.offer = None\n",
        "        self.acceptance = None\n",
        "        self.reward = None\n",
        "\n",
        "    def calculate_reward(self):\n",
        "        '''Returns an array stating who has accepted/rejected the offer'''\n",
        "        return torch.tensor([1 if accept else -1 for accept in self.acceptance]).sum().item()\n",
        "\n",
        "\n",
        "    def reset(self,*, seed=None, options=None):\n",
        "\n",
        "        self.state = np.zeros(2 * self.num_agents)\n",
        "        self.offer = None\n",
        "        self.acceptance = None\n",
        "        self.reward = None\n",
        "        return self.state, {} #observation, info_dict\n",
        "\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.offer = actions\n",
        "\n",
        "        self.acceptance = [action >= res_price for action, res_price in zip(actions, self.reservation_prices)]\n",
        "\n",
        "\n",
        "        self.reward = self.calculate_reward()\n",
        "\n",
        "        self.state = np.array([*actions, *self.acceptance])\n",
        "\n",
        "        done = all(self.acceptance)\n",
        "        terminateds = truncateds = done\n",
        "        return self.state, self.reward, terminateds, truncateds, {}\n",
        "\n"
      ],
      "metadata": {
        "id": "-mA7fh0OivEv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Register and run"
      ],
      "metadata": {
        "id": "waK7tNv1kwPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_env_gymnasium_rllib():\n",
        "    import ray\n",
        "    from ray import tune\n",
        "    from ray.tune.registry import register_env\n",
        "\n",
        "    if ray.is_initialized(): ray.shutdown()\n",
        "    ray.init(local_mode=True, include_dashboard=False, ignore_reinit_error=True)\n",
        "\n",
        "\n",
        "    # Custom env config\n",
        "    env_config = {\n",
        "        'num_agents': 2,\n",
        "        'num_samples': 100,\n",
        "        'priors'     : [0.5, 0.5],\n",
        "        'reservation_prices': [30, 30]\n",
        "    }\n",
        "    def env_creator(env_config):\n",
        "        return MyCustomEnv_gymnasium(env_config)\n",
        "    register_env(\"my_custom_env\", env_creator)\n",
        "\n",
        "    # Trial config\n",
        "    model_config = {\n",
        "        \"env\": \"my_custom_env\",\n",
        "        \"framework\": \"torch\",\n",
        "        \"num_workers\": 0,\n",
        "        }\n",
        "\n",
        "    stop = {\n",
        "        \"timesteps_total\": 1\n",
        "    }\n",
        "\n",
        "    results = tune.run(\"PPO\", config=model_config, stop=stop)"
      ],
      "metadata": {
        "id": "qnk7JmBWjUI6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env_gymnasium_rllib()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l2UjnY1cjPBm",
        "outputId": "1563fe13-fe3c-4c60-d91b-0463b8228af9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-04 08:11:07,602\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "2023-08-04 08:11:09,748\tINFO tune.py:666 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "2023-08-04 08:11:15,289\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
            "/usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  pkg_resources.declare_namespace(__name__)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(parent)\n",
            "2023-08-04 08:11:16,045\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
            "2023-08-04 08:11:16,055\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
            "2023-08-04 08:11:16,152\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "2023-08-04 08:11:16,255\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "2023-08-04 08:11:16,334\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            ":job_id:01000000\n",
            "2023-08-04 08:11:16,439\tWARNING worker.py:854 -- `ray.get_gpu_ids()` will always return the empty list when called from the driver. This is because Ray does not manage GPU allocations to the driver process.\n",
            ":task_name:bundle_reservation_check_func\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------+\n",
            "| Configuration for experiment     PPO                   |\n",
            "+--------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator |\n",
            "| Scheduler                        FIFOScheduler         |\n",
            "| Number of trials                 1                     |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/PPO\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/PPO`\n",
            "\n",
            ":job_id:01000000\n",
            ":task_name:bundle_reservation_check_func\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ":actor_name:PPO\n",
            "2023-08-04 08:11:16,570\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "2023-08-04 08:11:16,574\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
            "2023-08-04 08:11:16,589\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
            "2023-08-04 08:11:16,681\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "2023-08-04 08:11:16,719\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
            "2023-08-04 08:11:16,724\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
            "2023-08-04 08:11:16,726\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
            "2023-08-04 08:11:16,732\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":actor_name:PPO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-04 08:11:16,913\tERROR actor.py:970 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=4986, ip=172.28.0.12, actor_id=abaeb2ab5bf99ae50b0282b301000000, repr=PPO)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 169, in __init__\n",
            "    self.setup(copy.deepcopy(self.config))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
            "    self.workers = WorkerSet(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 157, in __init__\n",
            "    self._setup(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 247, in _setup\n",
            "    self._local_worker = self._make_worker(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/worker_set.py\", line 925, in _make_worker\n",
            "    worker = cls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 525, in __init__\n",
            "    self._update_policy_map(policy_dict=self.policy_dict)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1727, in _update_policy_map\n",
            "    self._build_policy_map(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 1838, in _build_policy_map\n",
            "    new_policy = create_policy_for_framework(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
            "    return policy_class(observation_space, action_space, merged_config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 64, in __init__\n",
            "    self._initialize_loss_from_dummy_batch()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/policy.py\", line 1418, in _initialize_loss_from_dummy_batch\n",
            "    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/torch_policy_v2.py\", line 551, in compute_actions_from_input_dict\n",
            "    return self._compute_action_helper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/threading.py\", line 32, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n",
            "    return func(self, *a, **k)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/torch_policy_v2.py\", line 1235, in _compute_action_helper\n",
            "    logp = action_dist.logp(actions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/models/torch/torch_distributions.py\", line 321, in logp\n",
            "    logps = torch.stack([cat.log_prob(act) for cat, act in zip(self._cats, value)])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/models/torch/torch_distributions.py\", line 321, in <listcomp>\n",
            "    logps = torch.stack([cat.log_prob(act) for cat, act in zip(self._cats, value)])\n",
            "AttributeError: 'TorchCategorical' object has no attribute 'log_prob'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 1 PENDING\n",
            "Current time: 2023-08-04 08:11:17. Total running time: 0s\n",
            "Logical resource usage: 1.0/2 CPUs, 0/0 GPUs\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_my_custom_env_7c146_00000   PENDING  |\n",
            "+------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-04 08:11:23,667\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 1 PENDING\n",
            "Current time: 2023-08-04 08:11:23. Total running time: 7s\n",
            "Logical resource usage: 1.0/2 CPUs, 0/0 GPUs\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_my_custom_env_7c146_00000   PENDING  |\n",
            "+------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a8db374edc87>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_env_gymnasium_rllib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-99990e8a6ac5>\u001b[0m in \u001b[0;36mtest_env_gymnasium_rllib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     }\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PPO\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0mexperiment_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_state_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0mincomplete_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/execution/trial_runner.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;34m\"\"\"Cleanup trials and callbacks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cleanup_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_experiment_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/execution/tune_controller.py\u001b[0m in \u001b[0;36m_cleanup_trials\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m                     \u001b[0;34m\"Waiting for actor manager to clean up final state [dedup]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 )\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Force cleanup of remaining actors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/actor_manager.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mstart_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_futures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_returns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\u001b[0m in \u001b[0;36mauto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mauto_init_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m         \u001b[0mtimeout_milliseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m         ready_ids, remaining_ids = worker.core_worker.wait(\n\u001b[0m\u001b[1;32m   2729\u001b[0m             \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNMyvgl_lmr_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}