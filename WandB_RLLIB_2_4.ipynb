{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP4WSD/HQWv7RyueGN3J1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcipolina/Ray_tutorials/blob/main/WandB_RLLIB_2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CvE4ovepRa9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0954180-9332-455f-c95b-359ad11788b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m836.9/836.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hray, version 2.6.2\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install ray[rllib] --quiet #ray, version 2.6.1\n",
        "! ray --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --quiet\n",
        "import wandb\n",
        "from ray.air.integrations.wandb import WandbLoggerCallback"
      ],
      "metadata": {
        "id": "CUi4jQ97jbdA",
        "outputId": "c7a564ae-88d6-4282-f82c-b8da7b73dab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP8N7at6k28j",
        "outputId": "5c5d27a5-9870-42a9-8335-86329302a6c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import ray\n",
        "#from ray.rllib.env import MultiAgentEnv #RLLIB 2.2\n",
        "from ray import air, tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "#from ray.rllib.env.multi_agent_env import MultiAgentEnv #new RLLIB 2.3\n",
        "#from ray.rllib.policy.policy import PolicySpec #For multi-policy mapping"
      ],
      "metadata": {
        "id": "B0gEOpEri308",
        "outputId": "5816372a-31f1-405a-db70-b2e168910533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "2023-08-09 09:13:07,907\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
            "/usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  pkg_resources.declare_namespace(__name__)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(parent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New Gym on RLLIB 2.3\n",
        "!pip install gymnasium --quiet\n",
        "import gymnasium as gym\n",
        "#from gym.spaces import MultiDiscrete\n",
        "#from gym.spaces import Tuple, Box, MultiDiscrete, Discrete"
      ],
      "metadata": {
        "id": "fyh6016jmV44",
        "outputId": "5d7d00b1-8c3c-4bc7-a24d-c3ad47308c37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure experiments and train"
      ],
      "metadata": {
        "id": "vSDivxmHjGQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.rllib.examples.env.two_step_game import TwoStepGame"
      ],
      "metadata": {
        "id": "PD4efhByWSF-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/ray-project/ray/blob/17596b03d189f2e6d642fb5690aefb01faec90d1/rllib/examples/centralized_critic_2.py\n",
        "\n",
        "#Simplified version\n",
        "\n",
        "config = PPOConfig()\\\n",
        "        .environment(TwoStepGame)\\\n",
        "        .framework(\"torch\")\\\n",
        "        .rollouts(batch_mode=\"complete_episodes\", num_rollout_workers=0)\n",
        ""
      ],
      "metadata": {
        "id": "SGyBjtzJLR8Z",
        "outputId": "ca60ba02-44b1-44da-a7bc-fd54ef84f0a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-09 09:13:20,426\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [WandbLoggerCallback(\n",
        "\tproject='WANDB_PROJECT',\n",
        "        log_config=True)]"
      ],
      "metadata": {
        "id": "cq-nskvWSwQp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE: it only works with local_mode = False!"
      ],
      "metadata": {
        "id": "xTNyhWhelTA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if ray.is_initialized(): ray.shutdown()\n",
        "ray.init(local_mode=False,include_dashboard=False, ignore_reinit_error=True)\n",
        "\n",
        "\n",
        "stop = {\n",
        "        \"training_iteration\": 1,\n",
        "        \"timesteps_total\": 1\n",
        "    }\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "        \"PPO\",\n",
        "        param_space=config.to_dict(),\n",
        "        run_config=air.RunConfig(  stop=stop, verbose=1, callbacks=callbacks\n",
        "                                 ),\n",
        "       )\n",
        "\n",
        "results = tuner.fit()\n",
        "\n",
        "# Get the best result based on a particular metric.\n",
        "best_result = results.get_best_result(metric=\"episode_reward_mean\", mode=\"max\") #TODO: en realidad, tengo que max por separado\n",
        "result_df = best_result.metrics_dataframe\n",
        "print(\"BEST RESULTS:\")\n",
        "print(result_df[[\"training_iteration\",'episode_reward_mean','episode_len_mean']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jztVLPvNPsde",
        "outputId": "3fd738ce-62fb-4eab-bda7-c17216bcb17e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-09 09:13:27,048\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "2023-08-09 09:13:32,580\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
            "2023-08-09 09:13:32,596\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
            "2023-08-09 09:13:32,747\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "2023-08-09 09:13:32,945\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "2023-08-09 09:13:33,020\tINFO tune.py:666 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
            "2023-08-09 09:13:33,117\tINFO wandb.py:305 -- Already logged into W&B.\n",
            "2023-08-09 09:13:33,141\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------+\n",
            "| Configuration for experiment     PPO                   |\n",
            "+--------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator |\n",
            "| Scheduler                        FIFOScheduler         |\n",
            "| Number of trials                 1                     |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/PPO\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/PPO`\n",
            "\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2023-08-09 09:13:33. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/0 GPUs\n",
            "+----------------------------------------+\n",
            "| Trial name                    status   |\n",
            "+----------------------------------------+\n",
            "| PPO_TwoStepGame_036a7_00000   PENDING  |\n",
            "+----------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1429)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=1429)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(pid=1429)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1429)\u001b[0m /usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "\u001b[2m\u001b[36m(pid=1429)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=1429)\u001b[0m   pkg_resources.declare_namespace(__name__)\n",
            "\u001b[2m\u001b[36m(pid=1429)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "\u001b[2m\u001b[36m(pid=1429)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=1429)\u001b[0m   declare_namespace(parent)\n",
            "\u001b[2m\u001b[36m(PPO pid=1429)\u001b[0m 2023-08-09 09:13:51,436\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "\u001b[2m\u001b[36m(PPO pid=1429)\u001b[0m 2023-08-09 09:13:51,436\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
            "\u001b[2m\u001b[36m(PPO pid=1429)\u001b[0m 2023-08-09 09:13:51,450\tWARNING env.py:56 -- Skipping env checking for this experiment\n",
            "\u001b[2m\u001b[36m(PPO pid=1429)\u001b[0m 2023-08-09 09:13:51,453\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "\u001b[2m\u001b[36m(PPO pid=1429)\u001b[0m 2023-08-09 09:13:51,536\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(PPO pid=1429)\u001b[0m 2023-08-09 09:13:51,536\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(PPO pid=1429)\u001b[0m 2023-08-09 09:13:51,536\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(PPO pid=1429)\u001b[0m 2023-08-09 09:13:51,536\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started with configuration:\n",
            "+---------------------------------------------------------------------------+\n",
            "| Training config                                                           |\n",
            "+---------------------------------------------------------------------------+\n",
            "| _AlgorithmConfig__prior_exploration_config/type        StochasticSampling |\n",
            "| _disable_action_flattening                                          False |\n",
            "| _disable_execution_plan_api                                          True |\n",
            "| _disable_initialize_loss_from_dummy_batch                           False |\n",
            "| _disable_preprocessor_api                                           False |\n",
            "| _enable_learner_api                                                  True |\n",
            "| _enable_rl_module_api                                                True |\n",
            "| _fake_gpus                                                          False |\n",
            "| _is_atari                                                                 |\n",
            "| _learner_class                                                            |\n",
            "| _tf_policy_handles_more_than_one_loss                               False |\n",
            "| action_mask_key                                               action_mask |\n",
            "| action_space                                                              |\n",
            "| actions_in_input_normalized                                         False |\n",
            "| always_attach_evaluation_results                                    False |\n",
            "| auto_wrap_old_gym_envs                                               True |\n",
            "| batch_mode                                              complete_episodes |\n",
            "| callbacks                                            ...efaultCallbacks'> |\n",
            "| checkpoint_trainable_policies_only                                  False |\n",
            "| clip_actions                                                        False |\n",
            "| clip_param                                                            0.3 |\n",
            "| clip_rewards                                                              |\n",
            "| compress_observations                                               False |\n",
            "| count_steps_by                                                  env_steps |\n",
            "| create_env_on_driver                                                False |\n",
            "| custom_eval_function                                                      |\n",
            "| delay_between_worker_restarts_s                                       60. |\n",
            "| disable_env_checking                                                False |\n",
            "| eager_max_retraces                                                     20 |\n",
            "| eager_tracing                                                        True |\n",
            "| enable_async_evaluation                                             False |\n",
            "| enable_connectors                                                    True |\n",
            "| enable_tf1_exec_eagerly                                             False |\n",
            "| entropy_coeff                                                          0. |\n",
            "| entropy_coeff_schedule                                                    |\n",
            "| env                                                  ...ame.TwoStepGame'> |\n",
            "| env_runner_cls                                                            |\n",
            "| env_task_fn                                                               |\n",
            "| evaluation_config                                                         |\n",
            "| evaluation_duration                                                    10 |\n",
            "| evaluation_duration_unit                                         episodes |\n",
            "| evaluation_interval                                                       |\n",
            "| evaluation_num_workers                                                  0 |\n",
            "| evaluation_parallel_to_training                                     False |\n",
            "| evaluation_sample_timeout_s                                          180. |\n",
            "| explore                                                              True |\n",
            "| export_native_model_files                                           False |\n",
            "| fake_sampler                                                        False |\n",
            "| framework                                                           torch |\n",
            "| gamma                                                                0.99 |\n",
            "| grad_clip                                                                 |\n",
            "| grad_clip_by                                                  global_norm |\n",
            "| ignore_worker_failures                                              False |\n",
            "| in_evaluation                                                       False |\n",
            "| input                                                             sampler |\n",
            "| keep_per_episode_custom_metrics                                     False |\n",
            "| kl_coeff                                                              0.2 |\n",
            "| kl_target                                                            0.01 |\n",
            "| lambda                                                                 1. |\n",
            "| local_gpu_idx                                                           0 |\n",
            "| local_tf_session_args/inter_op_parallelism_threads                      8 |\n",
            "| local_tf_session_args/intra_op_parallelism_threads                      8 |\n",
            "| log_level                                                            WARN |\n",
            "| log_sys_usage                                                        True |\n",
            "| logger_config                                                             |\n",
            "| logger_creator                                                            |\n",
            "| lr                                                                0.00005 |\n",
            "| lr_schedule                                                               |\n",
            "| max_num_worker_restarts                                              1000 |\n",
            "| max_requests_in_flight_per_sampler_worker                               2 |\n",
            "| metrics_episode_collection_timeout_s                                  60. |\n",
            "| metrics_num_episodes_for_smoothing                                    100 |\n",
            "| min_sample_timesteps_per_iteration                                      0 |\n",
            "| min_time_s_per_iteration                                                  |\n",
            "| min_train_timesteps_per_iteration                                       0 |\n",
            "| model/_disable_action_flattening                                    False |\n",
            "| model/_disable_preprocessor_api                                     False |\n",
            "| model/_time_major                                                   False |\n",
            "| model/_use_default_native_models                                       -1 |\n",
            "| model/always_check_shapes                                           False |\n",
            "| model/attention_dim                                                    64 |\n",
            "| model/attention_head_dim                                               32 |\n",
            "| model/attention_init_gru_gate_bias                                    2.0 |\n",
            "| model/attention_memory_inference                                       50 |\n",
            "| model/attention_memory_training                                        50 |\n",
            "| model/attention_num_heads                                               1 |\n",
            "| model/attention_num_transformer_units                                   1 |\n",
            "| model/attention_position_wise_mlp_dim                                  32 |\n",
            "| model/attention_use_n_prev_actions                                      0 |\n",
            "| model/attention_use_n_prev_rewards                                      0 |\n",
            "| model/conv_activation                                                relu |\n",
            "| model/conv_filters                                                        |\n",
            "| model/custom_action_dist                                                  |\n",
            "| model/custom_model                                                        |\n",
            "| model/custom_preprocessor                                                 |\n",
            "| model/dim                                                              84 |\n",
            "| model/encoder_latent_dim                                                  |\n",
            "| model/fcnet_activation                                               tanh |\n",
            "| model/fcnet_hiddens                                            [256, 256] |\n",
            "| model/framestack                                                     True |\n",
            "| model/free_log_std                                                  False |\n",
            "| model/grayscale                                                     False |\n",
            "| model/lstm_cell_size                                                  256 |\n",
            "| model/lstm_use_prev_action                                          False |\n",
            "| model/lstm_use_prev_action_reward                                      -1 |\n",
            "| model/lstm_use_prev_reward                                          False |\n",
            "| model/max_seq_len                                                      20 |\n",
            "| model/no_final_linear                                               False |\n",
            "| model/post_fcnet_activation                                          relu |\n",
            "| model/post_fcnet_hiddens                                               [] |\n",
            "| model/use_attention                                                 False |\n",
            "| model/use_lstm                                                      False |\n",
            "| model/vf_share_layers                                               False |\n",
            "| model/zero_mean                                                      True |\n",
            "| normalize_actions                                                    True |\n",
            "| num_consecutive_worker_failures_tolerance                             100 |\n",
            "| num_cpus_for_driver                                                     1 |\n",
            "| num_cpus_per_learner_worker                                             1 |\n",
            "| num_cpus_per_worker                                                     1 |\n",
            "| num_envs_per_worker                                                     1 |\n",
            "| num_gpus                                                                0 |\n",
            "| num_gpus_per_learner_worker                                             0 |\n",
            "| num_gpus_per_worker                                                     0 |\n",
            "| num_learner_workers                                                     0 |\n",
            "| num_sgd_iter                                                           30 |\n",
            "| num_workers                                                             0 |\n",
            "| observation_filter                                               NoFilter |\n",
            "| observation_fn                                                            |\n",
            "| observation_space                                                         |\n",
            "| offline_sampling                                                    False |\n",
            "| ope_split_batch_by_episode                                           True |\n",
            "| output                                                                    |\n",
            "| output_compress_columns                                ['obs', 'new_obs'] |\n",
            "| output_max_file_size                                             67108864 |\n",
            "| placement_strategy                                                   PACK |\n",
            "| policies/default_policy                              ...None, None, None) |\n",
            "| policies_to_train                                                         |\n",
            "| policy_map_cache                                                       -1 |\n",
            "| policy_map_capacity                                                   100 |\n",
            "| policy_mapping_fn                                    ...t 0x7d7c34466680> |\n",
            "| policy_states_are_swappable                                         False |\n",
            "| postprocess_inputs                                                  False |\n",
            "| preprocessor_pref                                                deepmind |\n",
            "| recreate_failed_workers                                             False |\n",
            "| remote_env_batch_wait_ms                                                0 |\n",
            "| remote_worker_envs                                                  False |\n",
            "| render_env                                                          False |\n",
            "| replay_sequence_length                                                    |\n",
            "| restart_failed_sub_environments                                     False |\n",
            "| rl_module_spec                                                            |\n",
            "| rollout_fragment_length                                              auto |\n",
            "| sample_async                                                        False |\n",
            "| sample_collector                                     ...leListCollector'> |\n",
            "| sampler_perf_stats_ema_coef                                               |\n",
            "| seed                                                                      |\n",
            "| sgd_minibatch_size                                                    128 |\n",
            "| shuffle_buffer_size                                                     0 |\n",
            "| shuffle_sequences                                                    True |\n",
            "| simple_optimizer                                                       -1 |\n",
            "| sync_filters_on_rollout_workers_timeout_s                             60. |\n",
            "| synchronize_filters                                                    -1 |\n",
            "| tf_session_args/allow_soft_placement                                 True |\n",
            "| tf_session_args/device_count/CPU                                        1 |\n",
            "| tf_session_args/gpu_options/allow_growth                             True |\n",
            "| tf_session_args/inter_op_parallelism_threads                            2 |\n",
            "| tf_session_args/intra_op_parallelism_threads                            2 |\n",
            "| tf_session_args/log_device_placement                                False |\n",
            "| torch_compile_learner                                               False |\n",
            "| torch_compile_learner_dynamo_backend                             inductor |\n",
            "| torch_compile_learner_dynamo_mode                                         |\n",
            "| torch_compile_learner_what_to_compile                ...ile.FORWARD_TRAIN |\n",
            "| torch_compile_worker                                                False |\n",
            "| torch_compile_worker_dynamo_backend                                onnxrt |\n",
            "| torch_compile_worker_dynamo_mode                                          |\n",
            "| train_batch_size                                                     4000 |\n",
            "| update_worker_filter_stats                                           True |\n",
            "| use_critic                                                           True |\n",
            "| use_gae                                                              True |\n",
            "| use_kl_loss                                                          True |\n",
            "| use_worker_filter_stats                                              True |\n",
            "| validate_workers_after_construction                                  True |\n",
            "| vf_clip_param                                                         10. |\n",
            "| vf_loss_coeff                                                          1. |\n",
            "| vf_share_layers                                                        -1 |\n",
            "| worker_cls                                                             -1 |\n",
            "| worker_health_probe_timeout_s                                          60 |\n",
            "| worker_restore_timeout_s                                             1800 |\n",
            "+---------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(PPO pid=1429)\u001b[0m Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 1 RUNNING\n",
            "Current time: 2023-08-09 09:14:03. Total running time: 30s\n",
            "Logical resource usage: 1.0/2 CPUs, 0/0 GPUs\n",
            "+----------------------------------------+\n",
            "| Trial name                    status   |\n",
            "+----------------------------------------+\n",
            "| PPO_TwoStepGame_036a7_00000   RUNNING  |\n",
            "+----------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m /usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m   pkg_resources.declare_namespace(__name__)\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m   declare_namespace(parent)\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Currently logged in as: lcipolina. Use `wandb login --relogin` to force relogin\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Tracking run with wandb version 0.15.8\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Run data is saved locally in /root/ray_results/PPO/PPO_TwoStepGame_036a7_00000_0_2023-08-09_09-13-33/wandb/run-20230809_091410-036a7_00000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Syncing run PPO_TwoStepGame_036a7_00000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: â­ï¸ View project at https://wandb.ai/lcipolina/WANDB_PROJECT\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: ğŸš€ View run at https://wandb.ai/lcipolina/WANDB_PROJECT/runs/036a7_00000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 1 RUNNING\n",
            "Current time: 2023-08-09 09:14:33. Total running time: 1min 0s\n",
            "Logical resource usage: 1.0/2 CPUs, 0/0 GPUs\n",
            "+----------------------------------------+\n",
            "| Trial name                    status   |\n",
            "+----------------------------------------+\n",
            "| PPO_TwoStepGame_036a7_00000   RUNNING  |\n",
            "+----------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(_QueueActor pid=1527)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(_QueueActor pid=1527)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(_QueueActor pid=1527)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(_QueueActor pid=1527)\u001b[0m /usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "\u001b[2m\u001b[36m(_QueueActor pid=1527)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(_QueueActor pid=1527)\u001b[0m   pkg_resources.declare_namespace(__name__)\n",
            "\u001b[2m\u001b[36m(_QueueActor pid=1527)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "\u001b[2m\u001b[36m(_QueueActor pid=1527)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(_QueueActor pid=1527)\u001b[0m   declare_namespace(parent)\n",
            "2023-08-09 09:15:00,537\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 9.648 s, which may be a performance bottleneck.\n",
            "2023-08-09 09:15:00,543\tWARNING util.py:315 -- The `process_trial_result` operation took 9.655 s, which may be a performance bottleneck.\n",
            "2023-08-09 09:15:00,546\tWARNING util.py:315 -- Processing trial results took 9.657 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
            "2023-08-09 09:15:00,547\tWARNING util.py:315 -- The `process_trial_result` operation took 9.660 s, which may be a performance bottleneck.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished iteration 1 at 2023-08-09 09:15:00. Total running time: 1min 27s\n",
            "+---------------------------------------------+\n",
            "| Training result                             |\n",
            "+---------------------------------------------+\n",
            "| episodes_total                         2000 |\n",
            "| num_env_steps_sampled                  4000 |\n",
            "| num_env_steps_trained                     0 |\n",
            "| sampler_results/episode_len_mean          2 |\n",
            "| sampler_results/episode_reward_mean   4.558 |\n",
            "+---------------------------------------------+\n",
            "\n",
            "Training saved a checkpoint for iteration 1 at: /root/ray_results/PPO/PPO_TwoStepGame_036a7_00000_0_2023-08-09_09-13-33/checkpoint_000001\n",
            "\n",
            "Training completed after 1 iterations at 2023-08-09 09:15:00. Total running time: 1min 27s\n",
            "\n",
            "Trial status: 1 TERMINATED\n",
            "Current time: 2023-08-09 09:15:00. Total running time: 1min 27s\n",
            "Logical resource usage: 1.0/2 CPUs, 0/0 GPUs\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                    status         iter     total time (s)     ts     reward     episode_reward_max     episode_reward_min     episode_len_mean     episodes_this_iter |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| PPO_TwoStepGame_036a7_00000   TERMINATED        1            58.7969   4000      4.558                      8                      0                    2                   2000 |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: \n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Run history:\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              agent_timesteps_total â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/_disable_action_flattening â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/_disable_execution_plan_api â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                   config/_disable_initialize_loss_from_dummy_batch â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   config/_disable_preprocessor_api â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/_enable_learner_api â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/_enable_rl_module_api â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/_fake_gpus â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       config/_tf_policy_handles_more_than_one_loss â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/actions_in_input_normalized â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/always_attach_evaluation_results â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                      config/auto_wrap_old_gym_envs â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/checkpoint_trainable_policies_only â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                config/clip_actions â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/clip_param â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/compress_observations â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/create_env_on_driver â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/delay_between_worker_restarts_s â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/disable_env_checking â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/eager_max_retraces â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/eager_tracing â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/enable_async_evaluation â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/enable_connectors â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/enable_tf1_exec_eagerly â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/entropy_coeff â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/evaluation_duration â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                      config/evaluation_num_workers â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/evaluation_parallel_to_training â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/evaluation_sample_timeout_s â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                     config/explore â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   config/export_native_model_files â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                config/fake_sampler â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                       config/gamma â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                      config/ignore_worker_failures â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/in_evaluation â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/keep_per_episode_custom_metrics â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                    config/kl_coeff â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                   config/kl_target â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                      config/lambda â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/local_gpu_idx â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:          config/local_tf_session_args/inter_op_parallelism_threads â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:          config/local_tf_session_args/intra_op_parallelism_threads â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/log_sys_usage â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                          config/lr â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/max_num_worker_restarts â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                   config/max_requests_in_flight_per_sampler_worker â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                        config/metrics_episode_collection_timeout_s â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/metrics_num_episodes_for_smoothing â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/min_sample_timesteps_per_iteration â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           config/min_train_timesteps_per_iteration â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/model/_disable_action_flattening â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/model/_disable_preprocessor_api â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/model/_time_major â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/model/_use_default_native_models â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   config/model/always_check_shapes â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/model/attention_dim â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    config/model/attention_head_dim â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/model/attention_init_gru_gate_bias â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/model/attention_memory_inference â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/model/attention_memory_training â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   config/model/attention_num_heads â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       config/model/attention_num_transformer_units â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       config/model/attention_position_wise_mlp_dim â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/model/attention_use_n_prev_actions â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/model/attention_use_n_prev_rewards â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                   config/model/dim â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/model/framestack â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/model/free_log_std â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                             config/model/grayscale â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/model/lstm_cell_size â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/model/lstm_use_prev_action â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           config/model/lstm_use_prev_action_reward â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/model/lstm_use_prev_reward â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/model/max_seq_len â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/model/no_final_linear â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/model/use_attention â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              config/model/use_lstm â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/model/vf_share_layers â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                             config/model/zero_mean â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/normalize_actions â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                   config/num_consecutive_worker_failures_tolerance â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_cpus_for_driver â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/num_cpus_per_learner_worker â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_cpus_per_worker â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_envs_per_worker â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                    config/num_gpus â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/num_gpus_per_learner_worker â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_gpus_per_worker â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_learner_workers â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                config/num_sgd_iter â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 config/num_workers â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/offline_sampling â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/ope_split_batch_by_episode â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/output_max_file_size â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/policy_map_cache â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/policy_map_capacity â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/policy_states_are_swappable â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/postprocess_inputs â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/recreate_failed_workers â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    config/remote_env_batch_wait_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/remote_worker_envs â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/render_env â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/restart_failed_sub_environments â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                config/sample_async â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/sgd_minibatch_size â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/shuffle_buffer_size â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/shuffle_sequences â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/simple_optimizer â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                   config/sync_filters_on_rollout_workers_timeout_s â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/synchronize_filters â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                        config/tf_session_args/allow_soft_placement â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/tf_session_args/device_count/CPU â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                    config/tf_session_args/gpu_options/allow_growth â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                config/tf_session_args/inter_op_parallelism_threads â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                config/tf_session_args/intra_op_parallelism_threads â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                        config/tf_session_args/log_device_placement â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/torch_compile_learner â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/torch_compile_worker â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/train_batch_size â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/update_worker_filter_stats â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/use_critic â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                     config/use_gae â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 config/use_kl_loss â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/use_worker_filter_stats â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                         config/validate_workers_after_construction â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/vf_clip_param â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/vf_loss_coeff â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                             config/vf_share_layers â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/worker_cls â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                               config/worker_health_probe_timeout_s â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    config/worker_restore_timeout_s â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                      connector_metrics/ObsPreprocessorConnector_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          connector_metrics/StateBufferConnector_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                 connector_metrics/ViewRequirementAgentConnector_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   counters/num_agent_steps_sampled â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   counters/num_agent_steps_trained â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     counters/num_env_steps_sampled â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     counters/num_env_steps_trained â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                   episode_len_mean â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 episode_reward_max â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                episode_reward_mean â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 episode_reward_min â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 episodes_this_iter â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                     episodes_total â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       info/learner/__all__/num_agent_steps_trained â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                         info/learner/__all__/num_env_steps_trained â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    info/learner/__all__/total_loss â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                     info/learner/default_policy/curr_entropy_coeff â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          info/learner/default_policy/curr_kl_coeff â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                info/learner/default_policy/curr_lr â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                info/learner/default_policy/entropy â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           info/learner/default_policy/mean_kl_loss â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            info/learner/default_policy/policy_loss â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             info/learner/default_policy/total_loss â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       info/learner/default_policy/vf_explained_var â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                info/learner/default_policy/vf_loss â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                      info/learner/default_policy/vf_loss_unclipped â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       info/num_agent_steps_sampled â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       info/num_agent_steps_trained â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         info/num_env_steps_sampled â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         info/num_env_steps_trained â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           iterations_since_restore â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            num_agent_steps_sampled â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            num_agent_steps_trained â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              num_env_steps_sampled â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    num_env_steps_sampled_this_iter â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           num_env_steps_sampled_throughput_per_sec â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              num_env_steps_trained â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    num_env_steps_trained_this_iter â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           num_env_steps_trained_throughput_per_sec â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                num_faulty_episodes â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                num_healthy_workers â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           num_in_flight_async_reqs â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         num_remote_worker_restarts â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        num_steps_trained_this_iter â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              perf/cpu_util_percent â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              perf/ram_util_percent â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             sampler_perf/mean_action_processing_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    sampler_perf/mean_env_render_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                      sampler_perf/mean_env_wait_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     sampler_perf/mean_inference_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            sampler_perf/mean_raw_obs_processing_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:      sampler_results/connector_metrics/ObsPreprocessorConnector_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:          sampler_results/connector_metrics/StateBufferConnector_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: sampler_results/connector_metrics/ViewRequirementAgentConnector_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   sampler_results/episode_len_mean â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 sampler_results/episode_reward_max â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                sampler_results/episode_reward_mean â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 sampler_results/episode_reward_min â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 sampler_results/episodes_this_iter â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                sampler_results/num_faulty_episodes â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:             sampler_results/sampler_perf/mean_action_processing_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                    sampler_results/sampler_perf/mean_env_render_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                      sampler_results/sampler_perf/mean_env_wait_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                     sampler_results/sampler_perf/mean_inference_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:            sampler_results/sampler_perf/mean_raw_obs_processing_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 time_since_restore â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                   time_this_iter_s â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                       time_total_s â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              timers/sample_time_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       timers/synch_weights_time_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  timers/training_iteration_time_ms â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                          timestamp â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                    timesteps_total â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 training_iteration â–\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: \n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Run summary:\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              agent_timesteps_total 8000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/_disable_action_flattening False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/_disable_execution_plan_api True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                   config/_disable_initialize_loss_from_dummy_batch False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   config/_disable_preprocessor_api False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/_enable_learner_api True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/_enable_rl_module_api True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/_fake_gpus False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       config/_tf_policy_handles_more_than_one_loss False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/actions_in_input_normalized False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/always_attach_evaluation_results False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                      config/auto_wrap_old_gym_envs True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/checkpoint_trainable_policies_only False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                config/clip_actions False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/clip_param 0.3\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/compress_observations False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/create_env_on_driver False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/delay_between_worker_restarts_s 60.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/disable_env_checking False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/eager_max_retraces 20\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/eager_tracing True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/enable_async_evaluation False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/enable_connectors True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/enable_tf1_exec_eagerly False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/entropy_coeff 0.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/evaluation_duration 10\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                      config/evaluation_num_workers 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/evaluation_parallel_to_training False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/evaluation_sample_timeout_s 180.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                     config/explore True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   config/export_native_model_files False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                config/fake_sampler False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                       config/gamma 0.99\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                      config/ignore_worker_failures False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/in_evaluation False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/keep_per_episode_custom_metrics False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                    config/kl_coeff 0.2\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                   config/kl_target 0.01\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                      config/lambda 1.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/local_gpu_idx 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:          config/local_tf_session_args/inter_op_parallelism_threads 8\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:          config/local_tf_session_args/intra_op_parallelism_threads 8\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/log_sys_usage True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                          config/lr 5e-05\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/max_num_worker_restarts 1000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                   config/max_requests_in_flight_per_sampler_worker 2\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                        config/metrics_episode_collection_timeout_s 60.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/metrics_num_episodes_for_smoothing 100\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/min_sample_timesteps_per_iteration 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           config/min_train_timesteps_per_iteration 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/model/_disable_action_flattening False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/model/_disable_preprocessor_api False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/model/_time_major False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/model/_use_default_native_models -1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   config/model/always_check_shapes False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/model/attention_dim 64\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    config/model/attention_head_dim 32\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/model/attention_init_gru_gate_bias 2.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/model/attention_memory_inference 50\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/model/attention_memory_training 50\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   config/model/attention_num_heads 1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       config/model/attention_num_transformer_units 1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       config/model/attention_position_wise_mlp_dim 32\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/model/attention_use_n_prev_actions 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          config/model/attention_use_n_prev_rewards 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                   config/model/dim 84\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/model/framestack True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/model/free_log_std False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                             config/model/grayscale False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/model/lstm_cell_size 256\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/model/lstm_use_prev_action False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           config/model/lstm_use_prev_action_reward -1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/model/lstm_use_prev_reward False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/model/max_seq_len 20\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/model/no_final_linear False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/model/use_attention False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              config/model/use_lstm False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/model/vf_share_layers False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                             config/model/zero_mean True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/normalize_actions True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                   config/num_consecutive_worker_failures_tolerance 100\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_cpus_for_driver 1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/num_cpus_per_learner_worker 1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_cpus_per_worker 1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_envs_per_worker 1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                    config/num_gpus 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/num_gpus_per_learner_worker 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_gpus_per_worker 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/num_learner_workers 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                config/num_sgd_iter 30\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 config/num_workers 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/offline_sampling False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/ope_split_batch_by_episode True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/output_max_file_size 67108864\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/policy_map_cache -1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/policy_map_capacity 100\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 config/policy_states_are_swappable False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/postprocess_inputs False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/recreate_failed_workers False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    config/remote_env_batch_wait_ms 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/remote_worker_envs False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/render_env False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             config/restart_failed_sub_environments False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                config/sample_async False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                          config/sgd_minibatch_size 128\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/shuffle_buffer_size 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           config/shuffle_sequences True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/simple_optimizer False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                   config/sync_filters_on_rollout_workers_timeout_s 60.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         config/synchronize_filters -1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                        config/tf_session_args/allow_soft_placement True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            config/tf_session_args/device_count/CPU 1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                    config/tf_session_args/gpu_options/allow_growth True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                config/tf_session_args/inter_op_parallelism_threads 2\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                config/tf_session_args/intra_op_parallelism_threads 2\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                        config/tf_session_args/log_device_placement False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       config/torch_compile_learner False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        config/torch_compile_worker False\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            config/train_batch_size 4000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  config/update_worker_filter_stats True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/use_critic True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                     config/use_gae True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 config/use_kl_loss True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     config/use_worker_filter_stats True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                         config/validate_workers_after_construction True\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/vf_clip_param 10.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                               config/vf_loss_coeff 1.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                             config/vf_share_layers -1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                  config/worker_cls -1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                               config/worker_health_probe_timeout_s 60\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    config/worker_restore_timeout_s 1800\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                      connector_metrics/ObsPreprocessorConnector_ms 0.05155\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          connector_metrics/StateBufferConnector_ms 0.04456\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                 connector_metrics/ViewRequirementAgentConnector_ms 1.13085\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   counters/num_agent_steps_sampled 8000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   counters/num_agent_steps_trained 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     counters/num_env_steps_sampled 4000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     counters/num_env_steps_trained 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                   episode_len_mean 2.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 episode_reward_max 8.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                episode_reward_mean 4.558\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 episode_reward_min 0.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 episodes_this_iter 2000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                     episodes_total 2000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       info/learner/__all__/num_agent_steps_trained 128.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                         info/learner/__all__/num_env_steps_trained 8000.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    info/learner/__all__/total_loss 2.00061\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                     info/learner/default_policy/curr_entropy_coeff 0.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                          info/learner/default_policy/curr_kl_coeff 0.3\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                info/learner/default_policy/curr_lr 5e-05\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                info/learner/default_policy/entropy 0.67286\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           info/learner/default_policy/mean_kl_loss 0.02641\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            info/learner/default_policy/policy_loss -0.0983\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             info/learner/default_policy/total_loss 2.00061\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                       info/learner/default_policy/vf_explained_var 0.22728\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                info/learner/default_policy/vf_loss 2.09363\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                      info/learner/default_policy/vf_loss_unclipped 2.11296\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       info/num_agent_steps_sampled 8000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       info/num_agent_steps_trained 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         info/num_env_steps_sampled 4000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         info/num_env_steps_trained 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           iterations_since_restore 1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            num_agent_steps_sampled 8000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                            num_agent_steps_trained 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              num_env_steps_sampled 4000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    num_env_steps_sampled_this_iter 4000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           num_env_steps_sampled_throughput_per_sec 68.07042\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              num_env_steps_trained 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    num_env_steps_trained_this_iter 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                           num_env_steps_trained_throughput_per_sec 0.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                num_faulty_episodes 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                num_healthy_workers 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                           num_in_flight_async_reqs 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                         num_remote_worker_restarts 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                        num_steps_trained_this_iter 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              perf/cpu_util_percent 80.43333\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              perf/ram_util_percent 22.83571\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                             sampler_perf/mean_action_processing_ms 0.40238\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                    sampler_perf/mean_env_render_ms 0.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                      sampler_perf/mean_env_wait_ms 0.14247\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                     sampler_perf/mean_inference_ms 3.9983\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                            sampler_perf/mean_raw_obs_processing_ms 4.05576\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:      sampler_results/connector_metrics/ObsPreprocessorConnector_ms 0.05155\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:          sampler_results/connector_metrics/StateBufferConnector_ms 0.04456\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: sampler_results/connector_metrics/ViewRequirementAgentConnector_ms 1.13085\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                   sampler_results/episode_len_mean 2.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 sampler_results/episode_reward_max 8.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                sampler_results/episode_reward_mean 4.558\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 sampler_results/episode_reward_min 0.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                 sampler_results/episodes_this_iter 2000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                sampler_results/num_faulty_episodes 0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:             sampler_results/sampler_perf/mean_action_processing_ms 0.40238\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                    sampler_results/sampler_perf/mean_env_render_ms 0.0\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                      sampler_results/sampler_perf/mean_env_wait_ms 0.14247\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                     sampler_results/sampler_perf/mean_inference_ms 3.9983\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:            sampler_results/sampler_perf/mean_raw_obs_processing_ms 4.05576\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 time_since_restore 58.79692\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                   time_this_iter_s 58.79692\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                       time_total_s 58.79692\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                              timers/sample_time_ms 35695.403\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                       timers/synch_weights_time_ms 2.214\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                  timers/training_iteration_time_ms 58762.596\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                          timestamp 1691572490\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                    timesteps_total 4000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb:                                                 training_iteration 1\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: \n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: ğŸš€ View run PPO_TwoStepGame_036a7_00000 at: https://wandb.ai/lcipolina/WANDB_PROJECT/runs/036a7_00000\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1528)\u001b[0m wandb: Find logs at: ./wandb/run-20230809_091410-036a7_00000/logs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEST RESULTS:\n",
            "   training_iteration  episode_reward_mean  episode_len_mean\n",
            "0                   1                4.558               2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other things that didn't work"
      ],
      "metadata": {
        "id": "aUz9zLsSXnr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility #Don't know how this works. Aparently is passed to the config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVpTf8zBnRMy",
        "outputId": "151f8805-b06d-4048-8c1b-9f531ebd439e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from: can't read /var/mail/ray.rllib.env.wrappers.multi_agent_env_compatibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple MARL custom Environment\n",
        "\n",
        "'''Simple Multi-agent environment to test things\n",
        "'''\n",
        "\n",
        "\n",
        "class TurnEnv(MultiAgentEnv):\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        super().__init__()\n",
        "        self.num_agents              = 2\n",
        "        self.t                       = 0\n",
        "        self.agent_lst               = list(range(self.num_agents))\n",
        "        self._agent_ids              = set(self.agent_lst)\n",
        "\n",
        "        # Now RLLIB needs both 'terminated' and 'truncated' to reset the environment\n",
        "        self.terminateds             = {agent: False for agent in self.agent_lst} #new RLLIB 2.3\n",
        "        self.terminateds['__all__']  = False\n",
        "        self.truncateds              = {agent: False for agent in self.agent_lst} #new RLLIB 2.3\n",
        "        self.truncateds['__all__']   = False\n",
        "        self.info_dict               = {}\n",
        "\n",
        "        self.observation_space       = Discrete(self.num_agents)\n",
        "        self.action_space            = self.observation_space\n",
        "\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        agentID = 0\n",
        "        self.info_dict = {agentID:\n",
        "                     {\"done\": self.terminateds[\"__all__\"]}\n",
        "                     } #new RLLIB 2.3\n",
        "        return {agentID: self.observation_space.sample()},self.info_dict # {agent:obs}, {agent:info}\n",
        "\n",
        "\n",
        "    def step(self, action_dict):\n",
        "\n",
        "        self.t +=1\n",
        "\n",
        "        # TEST\n",
        "        #if not action_dict:\n",
        "        #   print(\"EMPTY ACTION DICT!!!\")\n",
        "        #   print('self.t =', self.t)\n",
        "\n",
        "        if self.t == 10:\n",
        "           self.truncateds             = {agent: True for agent in self.agent_lst}\n",
        "           self.truncateds['__all__']  = True\n",
        "\n",
        "\n",
        "        agentID = 1\n",
        "        return {agentID:self.observation_space.sample()},\\\n",
        "               {agentID:1},\\\n",
        "               self.terminateds, \\\n",
        "               self.truncateds,\\\n",
        "               self.info_dict\n",
        "               #obs, rewards, terminateds, truncateds, infos\n"
      ],
      "metadata": {
        "id": "KgMyTkDsjDHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.spaces import Dict, Discrete, Tuple, MultiDiscrete\n",
        "from ray.tune import register_env\n",
        "from ray.tune.registry import get_trainable_cls"
      ],
      "metadata": {
        "id": "IRrraxxLEMhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/ray-project/ray/blob/master/rllib/examples/env/two_step_game.py\n",
        "\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE\n",
        "\n",
        "class A_TwoStepGame(MultiAgentEnv):\n",
        "    action_space = Discrete(2)\n",
        "\n",
        "    def __init__(self, env_config):\n",
        "        super().__init__()\n",
        "        self.action_space = Discrete(2)\n",
        "        self.state = None\n",
        "        self.agent_1 = 0\n",
        "        self.agent_2 = 1\n",
        "        self._skip_env_checking = True\n",
        "        # MADDPG emits action logits instead of actual discrete actions\n",
        "        self.actions_are_logits = env_config.get(\"actions_are_logits\", False)\n",
        "        self.one_hot_state_encoding = env_config.get(\"one_hot_state_encoding\", False)\n",
        "        self.with_state = env_config.get(\"separate_state_space\", False)\n",
        "        self._agent_ids = {0, 1}\n",
        "        if not self.one_hot_state_encoding:\n",
        "            self.observation_space = Discrete(6)\n",
        "            self.with_state = False\n",
        "        else:\n",
        "            # Each agent gets the full state (one-hot encoding of which of the\n",
        "            # three states are active) as input with the receiving agent's\n",
        "            # ID (1 or 2) concatenated onto the end.\n",
        "            if self.with_state:\n",
        "                self.observation_space = Dict(\n",
        "                    {\n",
        "                        \"obs\": MultiDiscrete([2, 2, 2, 3]),\n",
        "                        ENV_STATE: MultiDiscrete([2, 2, 2]),\n",
        "                    }\n",
        "                )\n",
        "            else:\n",
        "                self.observation_space = MultiDiscrete([2, 2, 2, 3])\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.state = np.array([1, 0, 0])\n",
        "        return self._obs(), {}\n",
        "\n",
        "    def step(self, action_dict):\n",
        "        if self.actions_are_logits:\n",
        "            action_dict = {\n",
        "                k: np.random.choice([0, 1], p=v) for k, v in action_dict.items()\n",
        "            }\n",
        "\n",
        "        state_index = np.flatnonzero(self.state)\n",
        "        if state_index == 0:\n",
        "            action = action_dict[self.agent_1]\n",
        "            assert action in [0, 1], action\n",
        "            if action == 0:\n",
        "                self.state = np.array([0, 1, 0])\n",
        "            else:\n",
        "                self.state = np.array([0, 0, 1])\n",
        "            global_rew = 0\n",
        "            terminated = False\n",
        "        elif state_index == 1:\n",
        "            global_rew = 7\n",
        "            terminated = True\n",
        "        else:\n",
        "            if action_dict[self.agent_1] == 0 and action_dict[self.agent_2] == 0:\n",
        "                global_rew = 0\n",
        "            elif action_dict[self.agent_1] == 1 and action_dict[self.agent_2] == 1:\n",
        "                global_rew = 8\n",
        "            else:\n",
        "                global_rew = 1\n",
        "            terminated = True\n",
        "\n",
        "        rewards = {self.agent_1: global_rew / 2.0, self.agent_2: global_rew / 2.0}\n",
        "        obs = self._obs()\n",
        "        terminateds = {\"__all__\": terminated}\n",
        "        truncateds = {\"__all__\": False}\n",
        "        infos = {\n",
        "            self.agent_1: {\"done\": terminateds[\"__all__\"]},\n",
        "            self.agent_2: {\"done\": terminateds[\"__all__\"]},\n",
        "        }\n",
        "        return obs, rewards, terminateds, truncateds, infos\n",
        "\n",
        "    def _obs(self):\n",
        "        if self.with_state:\n",
        "            return {\n",
        "                self.agent_1: {\"obs\": self.agent_1_obs(), ENV_STATE: self.state},\n",
        "                self.agent_2: {\"obs\": self.agent_2_obs(), ENV_STATE: self.state},\n",
        "            }\n",
        "        else:\n",
        "            return {self.agent_1: self.agent_1_obs(), self.agent_2: self.agent_2_obs()}\n",
        "\n",
        "    def agent_1_obs(self):\n",
        "        if self.one_hot_state_encoding:\n",
        "            return np.concatenate([self.state, [1]])\n",
        "        else:\n",
        "            return np.flatnonzero(self.state)[0]\n",
        "\n",
        "    def agent_2_obs(self):\n",
        "        if self.one_hot_state_encoding:\n",
        "            return np.concatenate([self.state, [2]])\n",
        "        else:\n",
        "            return np.flatnonzero(self.state)[0] + 3\n"
      ],
      "metadata": {
        "id": "D_wx4ivjBvWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}