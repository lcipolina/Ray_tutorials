{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMq2JNo7vM8hn8A42yooZRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcipolina/Ray/blob/main/RLLIB_Ray2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial on how to run RLLIB with Tune and Ray 2.0\n",
        "\n",
        "* First thing, Ray and RLLIB will still be backwards compatible for some time. \n",
        "\n",
        "* The changes is mostly on how we configure the experiment and set the parameters of the net. Ray needs those two things separately:\n",
        "\n",
        "  **1) A Way to configure the global run (called an \"experiment\" in Ray).**\n",
        "  \n",
        "  For example, how many iterations, how many checkpoints, where to save results. This is done through a dictionary where the parameters are keywords and the values are chosen from a list of possibilities\n",
        "\n",
        " **2) A way to configure the net (i.e. RL algo).** \n",
        "\n",
        " This is model dependent. For example, PPO and QMIX will have different parameters. For this, you import the \"configuration class\" and you can either use their default values or set your own values."
      ],
      "metadata": {
        "id": "LS00NSVWkNlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a simple environment\n",
        "The steps are similar to a gym environment:\n",
        "\n",
        "1) Customized environments inherit from a Ray-defined base class and need to override certain methods. The input and outputs of these methods are given and can't be changed.\n",
        "\n",
        "2) Register the environment \n",
        "\n",
        "3) Configure RLLIB to run the experiment (we will use Tune combined with Ray, because it gives us more functionalities)"
      ],
      "metadata": {
        "id": "Ex13ag9GoNlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"ray[all]\" #--quiet\n",
        "import os\n",
        "os._exit(0)\n",
        "import gym\n",
        "import numpy as np\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.tune.registry import register_env"
      ],
      "metadata": {
        "id": "6jvk-SwalQxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Environment taken from here: https://discuss.ray.io/t/observation-space-not-provided-in-policyspec/6501/16\n",
        "\n",
        "class MyEnv(gym.Env):\n",
        "    def __init__(self, config=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.action_space = gym.spaces.Box(\n",
        "            low=-1, high=1, shape=(2,), dtype=np.float32)\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(42410,), dtype=np.float32)\n",
        "\n",
        "    def _next_observation(self):\n",
        "      obs = np.random.rand(42410)\n",
        "      return obs\n",
        "\n",
        "    def _take_action(self, action):\n",
        "      self._reward = 1\n",
        "\n",
        "    def step(self, action):\n",
        "        # Execute one time step within the environment\n",
        "        self._reward = 0\n",
        "        self._take_action(action)\n",
        "        done = False\n",
        "        obs = self._next_observation()\n",
        "        return obs, self._reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self._reward = 0\n",
        "        self.total_reward = 0\n",
        "        self.visualization = None\n",
        "        return self._next_observation()"
      ],
      "metadata": {
        "id": "sqVPKvKkprR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Driver code for training with RLLIB"
      ],
      "metadata": {
        "id": "g2uVhCjLrFoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#REGINSTER THE ENVIRONMENT\n",
        "\n",
        "env_name = 'my_env'\n",
        "tune.register_env(env_name, lambda env_ctx: MyEnv()) #the register_env needs a callable/iterable"
      ],
      "metadata": {
        "id": "Am_4Pmj1rKFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL: CONFIGURE THE PARAMETERS OF PPO\n",
        "\n",
        "# this is done via importing the PPO configuration class and setting its values. \n",
        "# The customized configuration is actually not needed, one can use the default setting for PPO. \n",
        "# here I am just showing how to do customized configuration of some parameters.\n",
        "\n",
        "# If one wants to be serious about PPO and RLLIB, it is necessary to understand exactly how RLLIB treats the different parameters\n",
        "# For example, it has an \"adaptive KL term\" - meaning that it adapts the weight on the KL component of the loss. \n",
        "\n",
        "# see here for all the PPO parameters: \n",
        "# https://chuacheowhuan.github.io/RLlib_trainer_config/\n",
        "# https://discuss.ray.io/t/rllib-ray-rllib-config-parameters-for-ppo/691\n",
        "\n",
        "N_CPUS = 4\n",
        "config = PPOConfig()\\\n",
        "    .training(lr=5e-3,num_sgd_iter=10, train_batch_size = 256)\\\n",
        "    .framework(\"torch\")\\\n",
        "    .rollouts(num_rollout_workers=1, horizon= 10, rollout_fragment_length=10)\\\n",
        "    .resources(num_gpus=0,num_cpus_per_worker=1)\\\n",
        "    .environment(env = env_name, env_config={\n",
        "                                     \"num_workers\":1}# N_CPUS - 1}, #env_confit: arguments passed to the Env + num_workers = # number of parallel workers\n",
        "                     )"
      ],
      "metadata": {
        "id": "izRpoRF-s8Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIALIZE RAY AND RUN\n",
        "\n",
        "# Remember: Ray is the general library, RLLIB is the library (on top of Ray) for RL experiments and Tune + Air are the libraries for Hyperparam tuning.\n",
        "# In this example I am running RLLIB with Tune. This is just to give us more functionalities in the case we want to do hyperparam sweeping. \n",
        "# But RLLIB can be run either by command line or without Tune (just with fixed parameters) in a much simpler way. I'm just showing the most advanced way.\n",
        "\n",
        "# Additional information on how to Run RLLIB with different modalities\n",
        "# https://docs.ray.io/en/latest/rllib/core-concepts.html\n",
        "# https://docs.ray.io/en/master/rllib/index.html\n",
        "# https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py\n",
        "\n"
      ],
      "metadata": {
        "id": "yFsg8oXWtDbu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    if ray.is_initialized(): ray.shutdown()\n",
        "    ray.init(include_dashboard=True, ignore_reinit_error=True,) #Prints the dashboard running on a local port\n",
        "    experiment_name = 'my_env_experiment'\n",
        "    tuner = tune.Tuner(\"PPO\", param_space=config.to_dict(), #to run with Tune\n",
        "                        run_config=air.RunConfig(\n",
        "                                name =  experiment_name,\n",
        "                                stop={\"timesteps_total\": 10},\n",
        "                                #verbose = 1\n",
        "                                checkpoint_config=air.CheckpointConfig(\n",
        "                                checkpoint_frequency=50, checkpoint_at_end=True\n",
        "                                 ),\n",
        "                                )\n",
        "                                  )\n",
        "\n",
        "    results = tuner.fit()\n",
        "    ray.shutdown()\n",
        "\n",
        "    # This is it! results will be printed on the screen and also, look for a folder called \"ray_results\""
      ],
      "metadata": {
        "id": "OnalBFZqtuHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other ways to run that also work:"
      ],
      "metadata": {
        "id": "Q2RZiPmHwCEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Run Tune with the config class (without the second experiment_config dict)\n",
        "    #https://docs.google.com/document/d/1a3UEHEz6_Jth9O_2GR9Qp9c9IFyD5dObHCNSBsRgPGU/edit#\n",
        "    results = tune.run('PPO', config=config.to_dict(), stop={\"timesteps_total\": 10 })\n",
        "\n",
        "# 2) Alternatively: https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py\n",
        "    tune.Tuner(\"PPO\", param_space=config.to_dict()).fit()\n",
        "\n",
        "# 3) Run Tune without \"Air\" and a second experiment_config dict\n",
        "    # Define experiment details\n",
        "    experiment_name = 'my_env_experiment'\n",
        "    # Confiugure the experiment\n",
        "    experiment_dict = {\n",
        "            'name': experiment_name,\n",
        "            'run_or_experiment': 'PPO',\n",
        "            \"stop\": {\n",
        "                \"timesteps_total\": 10\n",
        "            },\n",
        "            'checkpoint_freq': 20,\n",
        "            \"config\": config.to_dict() #to run with Tune - need to convert the config class back to a dict\n",
        "        }\n",
        "    #Run with Tune\n",
        "     results = tune.run(**experiment_dict)\n",
        "\n",
        "  # 4) Run the old style with the config as a dict (still compatible)\n",
        "    config = {\n",
        "        \"env\": env_name,\n",
        "        \"num_workers\": 1,  # parallelism\n",
        "        \"horizon\": 10,\n",
        "        \"rollout_fragment_length\": 10,\n",
        "        \"train_batch_size\": 256,\n",
        "    }\n",
        "    stop = {\n",
        "        \"timesteps_total\": 10\n",
        "    }\n",
        "    results = tune.run(\"PPO\", config=config, stop=stop)\n",
        "\n",
        "# 5) Run with Tune and Air (simple example from documentation):\n",
        "    tuner = tune.Tuner(\"PPO\", param_space=config, run_config=air.RunConfig(stop=stop)    )\n",
        "    results = tuner.fit()\n",
        "\n"
      ],
      "metadata": {
        "id": "gbPdo0fXwGVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}